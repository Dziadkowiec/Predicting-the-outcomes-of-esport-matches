{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOGISIC REGRESSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by importing necessary libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and take a look at the beautiful X matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team kpm</th>\n",
       "      <th>dragons</th>\n",
       "      <th>barons</th>\n",
       "      <th>towers</th>\n",
       "      <th>dpm</th>\n",
       "      <th>vspm</th>\n",
       "      <th>earned gpm</th>\n",
       "      <th>monsterkills_pm</th>\n",
       "      <th>cspm</th>\n",
       "      <th>goldat15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.076987</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>-0.090476</td>\n",
       "      <td>-0.695238</td>\n",
       "      <td>-134.718128</td>\n",
       "      <td>-0.185134</td>\n",
       "      <td>-27.306101</td>\n",
       "      <td>-0.968975</td>\n",
       "      <td>-1.398766</td>\n",
       "      <td>-872.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.046773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-1.266667</td>\n",
       "      <td>-157.622987</td>\n",
       "      <td>-0.214460</td>\n",
       "      <td>-59.177227</td>\n",
       "      <td>-1.279145</td>\n",
       "      <td>-1.704927</td>\n",
       "      <td>-851.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003039</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>-0.295833</td>\n",
       "      <td>-2.016667</td>\n",
       "      <td>-211.525307</td>\n",
       "      <td>-0.393220</td>\n",
       "      <td>-94.854949</td>\n",
       "      <td>-1.253363</td>\n",
       "      <td>-1.796630</td>\n",
       "      <td>-1027.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.019650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>705.006500</td>\n",
       "      <td>5.300050</td>\n",
       "      <td>93.700150</td>\n",
       "      <td>2.777322</td>\n",
       "      <td>3.850050</td>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.479667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>187.260467</td>\n",
       "      <td>1.957733</td>\n",
       "      <td>441.377333</td>\n",
       "      <td>3.706456</td>\n",
       "      <td>3.117767</td>\n",
       "      <td>1349.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>0.004700</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>302.183200</td>\n",
       "      <td>-0.090650</td>\n",
       "      <td>-32.767300</td>\n",
       "      <td>0.195975</td>\n",
       "      <td>-1.532400</td>\n",
       "      <td>11.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>0.701100</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>1698.845950</td>\n",
       "      <td>5.274850</td>\n",
       "      <td>900.492850</td>\n",
       "      <td>10.928177</td>\n",
       "      <td>9.999650</td>\n",
       "      <td>7614.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>-0.035333</td>\n",
       "      <td>-2.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>-126.338950</td>\n",
       "      <td>-1.799233</td>\n",
       "      <td>-85.683450</td>\n",
       "      <td>-0.548742</td>\n",
       "      <td>-1.591583</td>\n",
       "      <td>273.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>-0.124610</td>\n",
       "      <td>-3.523810</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>-0.190476</td>\n",
       "      <td>-556.476652</td>\n",
       "      <td>-1.759019</td>\n",
       "      <td>-121.471481</td>\n",
       "      <td>-0.745499</td>\n",
       "      <td>-2.804424</td>\n",
       "      <td>-222.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>0.029300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>-582.493133</td>\n",
       "      <td>0.739450</td>\n",
       "      <td>63.577533</td>\n",
       "      <td>0.237807</td>\n",
       "      <td>-0.458283</td>\n",
       "      <td>-1047.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1456 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      team kpm   dragons    barons     towers          dpm      vspm  \\\n",
       "0     0.076987  0.028571 -0.090476  -0.695238  -134.718128 -0.185134   \n",
       "1     0.046773  0.000000 -0.200000  -1.266667  -157.622987 -0.214460   \n",
       "2    -0.003039 -0.025000 -0.295833  -2.016667  -211.525307 -0.393220   \n",
       "3    -0.019650  0.000000 -1.000000   2.000000   705.006500  5.300050   \n",
       "4     0.479667  1.000000  1.000000   6.333333   187.260467  1.957733   \n",
       "...        ...       ...       ...        ...          ...       ...   \n",
       "1451  0.004700 -1.500000 -0.500000  -2.500000   302.183200 -0.090650   \n",
       "1452  0.701100  5.500000  2.000000  14.500000  1698.845950  5.274850   \n",
       "1453 -0.035333 -2.333333 -0.333333  -1.333333  -126.338950 -1.799233   \n",
       "1454 -0.124610 -3.523810 -0.285714  -0.190476  -556.476652 -1.759019   \n",
       "1455  0.029300  1.000000  0.666667   2.166667  -582.493133  0.739450   \n",
       "\n",
       "      earned gpm  monsterkills_pm      cspm     goldat15  \n",
       "0     -27.306101        -0.968975 -1.398766  -872.666667  \n",
       "1     -59.177227        -1.279145 -1.704927  -851.666667  \n",
       "2     -94.854949        -1.253363 -1.796630 -1027.604167  \n",
       "3      93.700150         2.777322  3.850050   154.000000  \n",
       "4     441.377333         3.706456  3.117767  1349.333333  \n",
       "...          ...              ...       ...          ...  \n",
       "1451  -32.767300         0.195975 -1.532400    11.500000  \n",
       "1452  900.492850        10.928177  9.999650  7614.500000  \n",
       "1453  -85.683450        -0.548742 -1.591583   273.833333  \n",
       "1454 -121.471481        -0.745499 -2.804424  -222.809524  \n",
       "1455   63.577533         0.237807 -0.458283 -1047.333333  \n",
       "\n",
       "[1456 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('masterfile done.csv')\n",
    "\n",
    "X = df.iloc[:, 1:-2]\n",
    "y = df['future result']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to divide the data into the training and test sets. Adittionaly I used the StandardScaler function to boost the performance of the regression.\n",
    "\n",
    "The fit_transform and transform methods are used to make sure that there is no data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify= y, random_state = 1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I set up the cross-validation and hyper parameter tuning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "param_grid = {'C': np.arange(0.01, 2, 200),\n",
    "            'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "            'solver': ['liblinear', 'lbfgs', 'saga'],\n",
    "            'fit_intercept': [True, False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is just fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.676959917477159\n",
      "Best hyperparameters:  {'C': np.float64(0.01), 'fit_intercept': True, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "140 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.67695992        nan 0.67178014 0.67434424 0.67520631 0.67520631\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67695992        nan 0.67695992 0.67521368 0.67521368 0.67521368\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_cv = GridSearchCV(logreg, param_grid=param_grid, cv=kf)\n",
    "logreg_cv.fit(X_train_scaled, y_train)\n",
    "score = logreg_cv.best_score_\n",
    "print(\"Accuracy: \", score)\n",
    "print(\"Best hyperparameters: \", logreg_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the model achives quite high accuracy of about 68% with the the best possible combination of hyperparameters. The errors above are just the result of testing every solver with every type of penalty. The lbfgs solver doesn't support l2 type of regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test if by throwing out some predictor the model's performance can be increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(s):\n",
    "    \"\"\"funcion that spits out every subset of a given set\"\"\"\n",
    "    all_sets = []\n",
    "    x = len(s)\n",
    "    for i in range(1 << x):\n",
    "        #print([s[j] for j in range(x) if (i & (1 << j))])\n",
    "        subsets = [s[j] for j in range(x) if (i & (1 << j))]\n",
    "        all_sets.append(subsets)\n",
    "    return(all_sets)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = powerset([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
    "list = []\n",
    "df = pd.read_csv('masterfile done.csv')\n",
    "for i in range(1, (len(sets))):\n",
    "    X = df.iloc[:, sets[i]]\n",
    "    y = df['future result']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify= y, random_state = 1)\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    param_grid = {'C': np.arange(0.001, 2, 200),\n",
    "            'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "            'solver': ['liblinear', 'lbfgs', 'saga'],\n",
    "            'fit_intercept': [True, False]}\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg_cv = GridSearchCV(logreg, param_grid=param_grid, cv=kf)\n",
    "    logreg_cv.fit(X_train_scaled, y_train)\n",
    "    score = logreg_cv.best_score_\n",
    "    list.append([score, sets[i], logreg_cv.best_params_])\n",
    "df_r = pd.DataFrame(list)\n",
    "df_r.to_csv('logreg_result_done.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
